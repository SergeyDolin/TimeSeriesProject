{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skorch in /Users/sergeidolin/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/sergeidolin/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages (from skorch) (1.23.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /Users/sergeidolin/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages (from skorch) (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/sergeidolin/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages (from skorch) (1.11.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/sergeidolin/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages (from skorch) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in /Users/sergeidolin/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages (from skorch) (4.65.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/sergeidolin/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages (from scikit-learn>=0.22.0->skorch) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sergeidolin/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages (from scikit-learn>=0.22.0->skorch) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "     fbeta_score,\n",
    "     confusion_matrix,\n",
    "     roc_auc_score,\n",
    "     roc_curve,\n",
    "     mean_squared_error,\n",
    "     precision_recall_curve,\n",
    "     PrecisionRecallDisplay,\n",
    "     recall_score,\n",
    "     precision_score\n",
    ")\n",
    "from scipy import stats as st\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from catboost import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score\n",
    ")\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.cuda.is_available()\n",
    "\n",
    "from skorch import *\n",
    "from skorch.callbacks import EpochScoring, EarlyStopping\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_STATE = 1220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./taxi.csv', index_col=[0], parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('1H').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_orders</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-01 00:00:00</th>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 01:00:00</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 02:00:00</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 03:00:00</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 04:00:00</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31 19:00:00</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31 20:00:00</th>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31 21:00:00</th>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31 22:00:00</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31 23:00:00</th>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4416 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     num_orders\n",
       "datetime                       \n",
       "2018-03-01 00:00:00         124\n",
       "2018-03-01 01:00:00          85\n",
       "2018-03-01 02:00:00          71\n",
       "2018-03-01 03:00:00          66\n",
       "2018-03-01 04:00:00          43\n",
       "...                         ...\n",
       "2018-08-31 19:00:00         136\n",
       "2018-08-31 20:00:00         154\n",
       "2018-08-31 21:00:00         159\n",
       "2018-08-31 22:00:00         223\n",
       "2018-08-31 23:00:00         205\n",
       "\n",
       "[4416 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(data, max_lag, rolling_mean_size):\n",
    "    data['year'] = data.index.year\n",
    "    data['month'] = data.index.month\n",
    "    data['day'] = data.index.day\n",
    "    data['dayofweek'] = data.index.dayofweek\n",
    "    \n",
    "    for lag in range(1, max_lag + 1):\n",
    "        data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)\n",
    "\n",
    "    data['rolling_mean'] = data['num_orders'].shift(1).rolling(rolling_mean_size).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/30cd9v_56sz_qrnt8mp_9df40000gn/T/ipykernel_5274/705408069.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)\n",
      "/var/folders/n5/30cd9v_56sz_qrnt8mp_9df40000gn/T/ipykernel_5274/705408069.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)\n",
      "/var/folders/n5/30cd9v_56sz_qrnt8mp_9df40000gn/T/ipykernel_5274/705408069.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)\n",
      "/var/folders/n5/30cd9v_56sz_qrnt8mp_9df40000gn/T/ipykernel_5274/705408069.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)\n",
      "/var/folders/n5/30cd9v_56sz_qrnt8mp_9df40000gn/T/ipykernel_5274/705408069.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)\n",
      "/var/folders/n5/30cd9v_56sz_qrnt8mp_9df40000gn/T/ipykernel_5274/705408069.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['rolling_mean'] = data['num_orders'].shift(1).rolling(rolling_mean_size).mean()\n"
     ]
    }
   ],
   "source": [
    "make_features(df, 100, 200)\n",
    "\n",
    "train, test = train_test_split(df, shuffle=False, test_size=0.1)\n",
    "train = train.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_df(df):\n",
    "    print('------------------------------')\n",
    "    print('| Информация о наборе данных |')\n",
    "    print('------------------------------')\n",
    "    df.info()\n",
    "    print('-----------------------------------------')\n",
    "    print('| Первые и последние 5 строчек датасета |')\n",
    "    print('-----------------------------------------')\n",
    "    display(df)\n",
    "    print('--------------------')\n",
    "    print('| Сумма дубликатов |')\n",
    "    print('--------------------')\n",
    "    print(df.duplicated().sum())\n",
    "    for i in df.select_dtypes(include='object').columns.to_list():\n",
    "        print('--------------------------------------')\n",
    "        print(f'| Уникальные значения признака {i} |')\n",
    "        print('--------------------------------------')\n",
    "        print(df[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| Информация о наборе данных |\n",
      "------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3774 entries, 2018-03-09 08:00:00 to 2018-08-13 13:00:00\n",
      "Freq: H\n",
      "Columns: 106 entries, num_orders to rolling_mean\n",
      "dtypes: float64(101), int64(5)\n",
      "memory usage: 3.1 MB\n",
      "-----------------------------------------\n",
      "| Первые и последние 5 строчек датасета |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_orders</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_92</th>\n",
       "      <th>lag_93</th>\n",
       "      <th>lag_94</th>\n",
       "      <th>lag_95</th>\n",
       "      <th>lag_96</th>\n",
       "      <th>lag_97</th>\n",
       "      <th>lag_98</th>\n",
       "      <th>lag_99</th>\n",
       "      <th>lag_100</th>\n",
       "      <th>rolling_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-09 08:00:00</th>\n",
       "      <td>36</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>55.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09 09:00:00</th>\n",
       "      <td>43</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>54.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09 10:00:00</th>\n",
       "      <td>43</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09 11:00:00</th>\n",
       "      <td>49</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>54.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09 12:00:00</th>\n",
       "      <td>24</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>49.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>54.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-13 09:00:00</th>\n",
       "      <td>137</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>111.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-13 10:00:00</th>\n",
       "      <td>156</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>112.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-13 11:00:00</th>\n",
       "      <td>144</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>112.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-13 12:00:00</th>\n",
       "      <td>92</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>112.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-13 13:00:00</th>\n",
       "      <td>119</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>178.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>112.115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3774 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     num_orders  year  month  day  dayofweek  lag_1  lag_2  \\\n",
       "datetime                                                                     \n",
       "2018-03-09 08:00:00          36  2018      3    9          4   16.0    4.0   \n",
       "2018-03-09 09:00:00          43  2018      3    9          4   36.0   16.0   \n",
       "2018-03-09 10:00:00          43  2018      3    9          4   43.0   36.0   \n",
       "2018-03-09 11:00:00          49  2018      3    9          4   43.0   43.0   \n",
       "2018-03-09 12:00:00          24  2018      3    9          4   49.0   43.0   \n",
       "...                         ...   ...    ...  ...        ...    ...    ...   \n",
       "2018-08-13 09:00:00         137  2018      8   13          0   91.0   39.0   \n",
       "2018-08-13 10:00:00         156  2018      8   13          0  137.0   91.0   \n",
       "2018-08-13 11:00:00         144  2018      8   13          0  156.0  137.0   \n",
       "2018-08-13 12:00:00          92  2018      8   13          0  144.0  156.0   \n",
       "2018-08-13 13:00:00         119  2018      8   13          0   92.0  144.0   \n",
       "\n",
       "                     lag_3  lag_4  lag_5  ...  lag_92  lag_93  lag_94  lag_95  \\\n",
       "datetime                                  ...                                   \n",
       "2018-03-09 08:00:00    1.0   30.0   31.0  ...    85.0    62.0    50.0    59.0   \n",
       "2018-03-09 09:00:00    4.0    1.0   30.0  ...    37.0    85.0    62.0    50.0   \n",
       "2018-03-09 10:00:00   16.0    4.0    1.0  ...    58.0    37.0    85.0    62.0   \n",
       "2018-03-09 11:00:00   36.0   16.0    4.0  ...    45.0    58.0    37.0    85.0   \n",
       "2018-03-09 12:00:00   43.0   36.0   16.0  ...    59.0    45.0    58.0    37.0   \n",
       "...                    ...    ...    ...  ...     ...     ...     ...     ...   \n",
       "2018-08-13 09:00:00   66.0   83.0  143.0  ...    79.0   123.0    85.0   149.0   \n",
       "2018-08-13 10:00:00   39.0   66.0   83.0  ...    65.0    79.0   123.0    85.0   \n",
       "2018-08-13 11:00:00   91.0   39.0   66.0  ...    80.0    65.0    79.0   123.0   \n",
       "2018-08-13 12:00:00  137.0   91.0   39.0  ...   150.0    80.0    65.0    79.0   \n",
       "2018-08-13 13:00:00  156.0  137.0   91.0  ...   178.0   150.0    80.0    65.0   \n",
       "\n",
       "                     lag_96  lag_97  lag_98  lag_99  lag_100  rolling_mean  \n",
       "datetime                                                                    \n",
       "2018-03-09 08:00:00    31.0    14.0     3.0    16.0     34.0        55.150  \n",
       "2018-03-09 09:00:00    59.0    31.0    14.0     3.0     16.0        54.710  \n",
       "2018-03-09 10:00:00    50.0    59.0    31.0    14.0      3.0        54.500  \n",
       "2018-03-09 11:00:00    62.0    50.0    59.0    31.0     14.0        54.360  \n",
       "2018-03-09 12:00:00    85.0    62.0    50.0    59.0     31.0        54.275  \n",
       "...                     ...     ...     ...     ...      ...           ...  \n",
       "2018-08-13 09:00:00   139.0    78.0    28.0    34.0     63.0       111.980  \n",
       "2018-08-13 10:00:00   149.0   139.0    78.0    28.0     34.0       112.095  \n",
       "2018-08-13 11:00:00    85.0   149.0   139.0    78.0     28.0       112.140  \n",
       "2018-08-13 12:00:00   123.0    85.0   149.0   139.0     78.0       112.265  \n",
       "2018-08-13 13:00:00    79.0   123.0    85.0   149.0    139.0       112.115  \n",
       "\n",
       "[3774 rows x 106 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "| Сумма дубликатов |\n",
      "--------------------\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "info_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = train.drop('num_orders', axis=1)\n",
    "target_train = train['num_orders']\n",
    "\n",
    "features_test = test.drop('num_orders', axis=1)\n",
    "target_test = test['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          feature        VIF\n",
      "0            year  32.290737\n",
      "1           month  24.153539\n",
      "2             day   2.680706\n",
      "3       dayofweek   1.400267\n",
      "4           lag_1   3.072847\n",
      "..            ...        ...\n",
      "100        lag_97   3.127772\n",
      "101        lag_98   3.125845\n",
      "102        lag_99   3.116723\n",
      "103       lag_100   3.085710\n",
      "104  rolling_mean  63.211573\n",
      "\n",
      "[105 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = features_train.columns\n",
    "\n",
    "# вычисление VIF для каждого признака\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(features_train.values, i) \\\n",
    "                          for i in range(len(features_train.columns))]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              year\n",
       "1             month\n",
       "104    rolling_mean\n",
       "Name: feature, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIF_features_drop = vif_data.query('VIF > 9.0')['feature']\n",
    "VIF_features_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_vif = features_train.drop(VIF_features_drop.values, axis=1)\n",
    "features_test_vif = features_test.drop(VIF_features_drop.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = torch.tensor(features_train_vif.values, dtype=torch.float32),\\\n",
    "                                   torch.tensor(features_test_vif.values, dtype=torch.float32),\\\n",
    "                                   torch.tensor(target_train.values, dtype=torch.float32),\\\n",
    "                                   torch.tensor(target_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_slr = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "X_test_slr = torch.tensor(scaler.transform(X_test), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(target, prediction, prediction_proba):\n",
    "    print(\"F-beta:\",fbeta_score(target,prediction,average='macro',beta=2))\n",
    "    print(\"AUC-ROC:\", roc_auc_score(target, prediction_proba[:, 1]))\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(target, prediction_proba[:, 1])\n",
    "    cm_matrix = pd.DataFrame(data=confusion_matrix(target, prediction), \n",
    "                                columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "                                index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "    tp = cm_matrix['Actual Positive:1']['Predict Positive:1']\n",
    "    fp = cm_matrix['Actual Positive:1']['Predict Negative:0']\n",
    "    fn = cm_matrix['Actual Negative:0']['Predict Positive:1']\n",
    "    tn = cm_matrix['Actual Negative:0']['Predict Negative:0']\n",
    "    print('Precision =', round(tp / (tp + fp), 3))\n",
    "    print('Recall = ', round(tp / (tp + fn), 3))\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "    sns.heatmap(cm_matrix, annot=True, fmt='d', \n",
    "                cmap=sns.diverging_palette(220, 10, as_cmap=True), ax=axes[0])\n",
    "    axes[1].plot([0, 1], [0, 1], linestyle='--')\n",
    "    axes[1].plot(fpr, tpr)\n",
    "\n",
    "\n",
    "    axes[0].title.set_text('Матрица ошибок')\n",
    "    axes[1].title.set_text('ROC-кривая')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_lyrs: list[int], output_dim: int, batch_norm=False, dropout=False):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.hidden_lyrs = hidden_lyrs\n",
    "        if batch_norm == False and dropout == False:\n",
    "            if len(hidden_lyrs) > 1:\n",
    "                self.syn_in = nn.Linear(input_dim, hidden_lyrs[0])\n",
    "                self.hidden_syns = [nn.Linear(hidden_lyrs[i], hidden_lyrs[i+1]) for i in range(0, len(hidden_lyrs)-1)]\n",
    "                self.syn_out = nn.Linear(hidden_lyrs[-1], output_dim)\n",
    "            else:\n",
    "                self.syn_in = nn.Linear(input_dim, hidden_lyrs[0])\n",
    "                self.hidden_syns = []\n",
    "                self.syn_out = nn.Linear(hidden_lyrs[0], output_dim)\n",
    "        elif batch_norm == True and dropout == False:\n",
    "            if len(hidden_lyrs) > 1:\n",
    "                self.syn_in = nn.Linear(input_dim, hidden_lyrs[0])\n",
    "                self.hidden_syns = [[nn.Linear(hidden_lyrs[i], hidden_lyrs[i+1]), nn.BatchNorm1d(hidden_lyrs[i+1])] for i in range(len(hidden_lyrs)-1)]\n",
    "                self.syn_out = nn.Linear(hidden_lyrs[-1], output_dim)\n",
    "            else:\n",
    "                self.syn_in = nn.Linear(input_dim, hidden_lyrs[0])\n",
    "                self.hidden_syns = []\n",
    "                self.syn_out = nn.Linear(hidden_lyrs[0], output_dim)\n",
    "        elif batch_norm == False and dropout == True:\n",
    "            if len(hidden_lyrs) > 1:\n",
    "                self.syn_in = nn.Linear(input_dim, hidden_lyrs[0])\n",
    "                self.hidden_syns = [[nn.Linear(hidden_lyrs[i], hidden_lyrs[i+1]), nn.Dropout(np.random.uniform(0.1, 0.5))] for i in range(len(hidden_lyrs)-1)]\n",
    "                self.syn_out = nn.Linear(hidden_lyrs[-1], output_dim)\n",
    "            else:\n",
    "                self.syn_in = nn.Linear(input_dim, hidden_lyrs[0])\n",
    "                self.hidden_syns = []\n",
    "                self.syn_out = nn.Linear(hidden_lyrs[0], output_dim)\n",
    "        else:\n",
    "            if len(hidden_lyrs) > 1:\n",
    "                self.syn_in = nn.Linear(input_dim, hidden_lyrs[0])\n",
    "                self.hidden_syns = [[nn.Linear(hidden_lyrs[i], hidden_lyrs[i+1]), nn.BatchNorm1d(hidden_lyrs[i+1]), nn.Dropout(np.random.uniform(0.1, 0.5))] for i in range(len(hidden_lyrs)-1)]\n",
    "                self.syn_out = nn.Linear(hidden_lyrs[-1], output_dim)\n",
    "            else:\n",
    "                self.syn_in = nn.Linear(input_dim, hidden_lyrs[0])\n",
    "                self.hidden_syns = []\n",
    "                self.syn_out = nn.Linear(hidden_lyrs[0], output_dim)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm == False and self.dropout == False:\n",
    "            out = self.relu(self.syn_in(x))\n",
    "            if len(self.hidden_syns) > 1: \n",
    "                for syn in self.hidden_syns:\n",
    "                    out = self.relu(syn(out))\n",
    "            return self.relu(self.syn_out(out))\n",
    "        elif self.batch_norm == True and self.dropout == False:\n",
    "            f_b = nn.BatchNorm1d(self.hidden_lyrs[0])\n",
    "            out = self.relu(f_b(self.syn_in(x)))\n",
    "            if len(self.hidden_syns) > 1:\n",
    "                for syn in self.hidden_syns:\n",
    "                    out = self.relu(syn[1](syn[0](out)))\n",
    "            return self.relu(self.syn_out(out))\n",
    "        elif self.batch_norm == False and self.dropout == True:\n",
    "            f_d = nn.Dropout(np.random.uniform(0.1, 0.5))\n",
    "            out = f_d(self.relu(self.syn_in(x)))\n",
    "            if len(self.hidden_syns) > 1:\n",
    "                for syn in self.hidden_syns:\n",
    "                    out = syn[1](self.relu(syn[0](out)))\n",
    "            return self.relu(self.syn_out(out))\n",
    "        else:\n",
    "            f_d = nn.Dropout(np.random.uniform(0.1, 0.5))\n",
    "            f_b = nn.BatchNorm1d(self.hidden_lyrs[0])\n",
    "            out = f_d(self.relu(f_b(self.syn_in(x))))\n",
    "            if len(self.hidden_syns) > 1:\n",
    "                for syn in self.hidden_syns:\n",
    "                    out = syn[2](self.relu(syn[1](syn[0](out))))\n",
    "            return self.relu(self.syn_out(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция train_model предназначена для обучения модели и принимает в качестве параметров:\n",
    "#   model       ->  Объект класса NeuralNet\n",
    "#   batch_size  ->  Размер батча\n",
    "#   optimizer   ->  Функция оптимизации\n",
    "#   X,Y         ->  Тензор признаков и целевой соответсвтенно\n",
    "#   epochs      ->  Количество эпох\n",
    "#   btnr, drpt  ->  BatchNormaliztion и Dropout соответсвтенно\n",
    "#\n",
    "# Функцией потерь является MSE, но в лог выводится RMSE\n",
    "def train_model(model: NeuralNet, batch_size: int, optimizer, \n",
    "                X: torch.Tensor, Y: torch.Tensor, epochs: int, btnr=False, drpt=False):\n",
    "    num_batches = ceil(len(X)/batch_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    accumulation_iteration = 5\n",
    "    for i in range(epochs):\n",
    "        order = np.random.permutation(len(X))\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_index = batch_idx * batch_size\n",
    "            \n",
    "\n",
    "            batch_indexes = order[start_index:start_index+batch_size]\n",
    "            X_batch = X[batch_indexes]\n",
    "            y_batch = Y[batch_indexes]\n",
    "\n",
    "            preds = model(X_batch).flatten()\n",
    "\n",
    "            loss = criterion(preds.ravel(), y_batch) / accumulation_iteration\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ((batch_idx + 1) % accumulation_iteration == 0) or (batch_idx + 1 == num_batches):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        if i % 100 == 0 or i == epochs-1:\n",
    "            print(\"Epoch: {} RMSE : {:.2f}\".format(i, torch.sqrt(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LinearRegression(n_jobs=10, copy_X=True)\n",
    "\n",
    "start_time_lr = time()\n",
    "\n",
    "model_lr.fit(features_train_vif, target_train)\n",
    "\n",
    "end_time_lr = time()\n",
    "\n",
    "lr_model_time_fit = (end_time_lr-start_time_lr) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучение модели: 0.00059 сек.\n"
     ]
    }
   ],
   "source": [
    "print('Время обучение модели: {0:.5f} сек.'. format(lr_model_time_fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_lr = time()\n",
    "predicted = model_lr.predict(features_train_vif)\n",
    "end_time_lr = time()\n",
    "\n",
    "lr_model_time_predict = (end_time_lr-start_time_lr) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_cv = LinearRegression(n_jobs=10, copy_X=True)\n",
    "\n",
    "start_time_lr = time()\n",
    "folds = KFold(n_splits = 4, shuffle = True, random_state = RANDOM_STATE)\n",
    "scores = cross_val_score(model_lr_cv, features_train_vif, target_train, scoring='neg_root_mean_squared_error', cv=4)\n",
    "end_time_lr = time()\n",
    "\n",
    "lr_model_time_cv = (end_time_lr-start_time_lr) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE кросс валидации: 18.49.\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE кросс валидации: {0:.2f}.\". \\\n",
    "    format(scores.max() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 39.11121197,  52.50018944,  41.45665162, ..., 110.36595152,\n",
       "       115.97329603, 112.47841392])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.predict(features_train_vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = len(features_train_vif.columns)\n",
    "HIDDEN = [75,50,25,10]\n",
    "OUTPUT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 RMSE : 39.46\n",
      "Epoch: 100 RMSE : 35.48\n",
      "Epoch: 200 RMSE : 41.73\n",
      "Epoch: 300 RMSE : 35.84\n",
      "Epoch: 400 RMSE : 36.83\n",
      "Epoch: 500 RMSE : 40.78\n",
      "Epoch: 600 RMSE : 36.01\n",
      "Epoch: 700 RMSE : 40.98\n",
      "Epoch: 800 RMSE : 39.80\n",
      "Epoch: 900 RMSE : 36.96\n",
      "Epoch: 999 RMSE : 39.03\n"
     ]
    }
   ],
   "source": [
    "model_1 = NeuralNet(input_dim=INPUT, hidden_lyrs=HIDDEN, output_dim=OUTPUT)\n",
    "                    \n",
    "optimizer = torch.optim.Adam(model_1.parameters(), lr=.01)\n",
    "train_model(model_1, 100, optimizer,X_train_slr,Y_train,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skorch_regressor = NeuralNetRegressor(module=model_1, \n",
    "                                      device=DEVICE,  \n",
    "                                      verbose=3,\n",
    "                                      batch_size=100,\n",
    "                                      optimizer=torch.optim.Adam, \n",
    "                                      max_epochs=2500,\n",
    "                                      lr=0.01\n",
    "                                      train_split=4,\n",
    "                                      criterion =nn.MSELoss,\n",
    "                                      callbacks=[\n",
    "                ('val_rmse', EpochScoring(scoring=\"neg_root_mean_squared_error\", lower_is_better=True, name='RMSE')),\n",
    "                ('estoper', EarlyStopping( lower_is_better=True, monitor='RMSE')),\n",
    "            ],\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "params = {\n",
    "    \n",
    "    'batch_norm': [True, False],\n",
    "    'dropout': [True, False],\n",
    "    'lr': [1e-2, 1e-3, 1e-4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'dropout' for estimator <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n  module=NeuralNet(\n    (relu): ReLU()\n    (syn_in): Linear(in_features=102, out_features=75, bias=True)\n    (syn_out): Linear(in_features=10, out_features=1, bias=True)\n  ),\n). Valid parameters are: ['module', 'criterion', 'optimizer', 'lr', 'max_epochs', 'batch_size', 'iterator_train', 'iterator_valid', 'dataset', 'train_split', 'callbacks', 'predict_nonlinearity', 'warm_start', 'verbose', 'device', 'compile', 'use_caching', '_params_to_validate'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m grid \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(skorch_regressor, params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_root_mean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \n\u001b[0;32m----> 2\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_slr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1753\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1753\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/joblib/parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/joblib/parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1784\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:674\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m parameters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    672\u001b[0m         cloned_parameters[k] \u001b[38;5;241m=\u001b[39m clone(v, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 674\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcloned_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    678\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/skorch/net.py:2084\u001b[0m, in \u001b[0;36mNeuralNet.set_params\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2081\u001b[0m         normal_params[key] \u001b[38;5;241m=\u001b[39m val\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_virtual_params(virtual_params)\n\u001b[0;32m-> 2084\u001b[0m \u001b[43mBaseEstimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnormal_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m special_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2087\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/sklearn/base.py:246\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_params:\n\u001b[1;32m    245\u001b[0m     local_valid_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names()\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for estimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid parameters are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_valid_params\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delim:\n\u001b[1;32m    252\u001b[0m     nested_params[key][sub_key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter 'dropout' for estimator <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n  module=NeuralNet(\n    (relu): ReLU()\n    (syn_in): Linear(in_features=102, out_features=75, bias=True)\n    (syn_out): Linear(in_features=10, out_features=1, bias=True)\n  ),\n). Valid parameters are: ['module', 'criterion', 'optimizer', 'lr', 'max_epochs', 'batch_size', 'iterator_train', 'iterator_valid', 'dataset', 'train_split', 'callbacks', 'predict_nonlinearity', 'warm_start', 'verbose', 'device', 'compile', 'use_caching', '_params_to_validate']."
     ]
    }
   ],
   "source": [
    "grid = RandomizedSearchCV(skorch_regressor, params, cv=3, scoring=\"neg_root_mean_squared_error\", verbose=3) \n",
    "grid.fit(X_train_slr, Y_train.reshape(-1,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_practicum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
